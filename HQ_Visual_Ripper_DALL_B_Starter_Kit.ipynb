{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "HQ Visual Ripper DALL-B Starter Kit",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O78RfTZfh7ji"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# HQ Ripper\n",
        "> * HQ Ripper Starter Kit 1.0a \n",
        "* Use with care\n",
        "* DALL-B is cute and needs cuddles \n",
        "She is shy so be nice :) \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aDcCMUnjQLh"
      },
      "source": [
        "\n",
        "#### What is this?\n",
        "\n",
        "This is a notebook that uses DALL-E's decoder and CLIP to generate images from text. I will very likely make this better & easier to use in the future.\n",
        "\n",
        "Feel free to send correspondence to [@advadnoun](https://twitter.com/advadnoun) on Twitter. If you use this notebook or modify it, please be cool and link back to my twitter. \n",
        "\n",
        "If you're feeling generous while your Waluigi (or whatever you choose) loads, you can donate to @rynnn on Venmo :)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt_zl5saZMlf"
      },
      "source": [
        "* Who might use this?\n",
        "> For memes\n",
        "> Wallpapers\n",
        "> .......... IDK "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQULmg_lpMNq"
      },
      "source": [
        "# How do I use this?\n",
        "\n",
        "First, type in your description where it says *Insert text here*.\n",
        "\n",
        "\n",
        "Second, click the ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD0AAAA/CAYAAABTqsDiAAAEhklEQVRoBeWbx07rUBCG82xsYQtbeBB4CPYUsWAHQkiwAIEQsEGigwDRe++9M1efpYkOTmIf7NjXZfHLCY6P/Z0pZ4zHhZ+fH/n+/i7R19eXuPX5+SlZUAFoU+4JyCL4L2gT2IR1W/fj40P89P7+LklVEbocsMKagG6Qt7c3SZsc6ErACmuCmoCvr6+SRhXcwKZ1gQUSsJeXF3l6enL0+PgoqoeHB0mbitAaw0ADa4ICCNj9/b3c3d3J7e2to5ubGzF1fX0taZADbQKrZZ+fn2Vvby8TcodgERoLa7wCjFUzC61WJmkBDTCufHV1lW1oM46JX+L05OQk29CmlXHr8/PzzAAToiUxjXsDzQ618vHxsWxsbGQGvARaXZt1WGP54OBAVldXswuNlVmXgca1Ly4uHNilpaV8QFN4EM87OzsyPz+ffWiWKqDPzs5ke3s7H9DU1UCfnp7K1taWzM7OZtvSFCW5heZGQi09MzMTmaWHhoakr69PWltbHfGZv0VV9pYsWVqYYGkqMaA3Nzel2tATExPS0tIidXV1UlNTU1bs4zf8tpoTUAKt98xRQbPeA1IJtNLfOaZatUKs0FisoaHhz8A6ERxbDavHBs3FermygvltGSMseCzQuGUYC7sngrHCuHos0M3NzYFd2g2s3xkzaHKLHBpX1AuttG1sbJTa2lrf37mPD7qiRA5tY2XWZyYHeDeY1/eg1o4c2saCQOOqxKnNJOlEkNSCuHik0FRVeoFeW4VWgO7ubmt3D1K5RQpNOekFq/vc0MDbujvn0Mmy3UYKDYyCeW3LQQNg4+6VjvWagMRD+5WsiYMO695NTU2+npI49w6TyGxL1sQlMuLqr0uWnzubuYGxvWK30r5IY5qT2qy7xCXZ2sadTejEFic2ZSiwtu5sQie2DLW1tglj8zmolbmeyN2bk7De1tfX+2ZiG1h+w1iJv7UEHDe3SWp+4IzBWIwZVLFYWi+Oiw1jcY4NC8y1xArNCW1Ky3LWJobDuLRO/H+B1pNjMUC8XJ59/CZoltZzubcllo7r/97mhVBVUU6yXiM+B6m0zDG9PntCx/WEw+sCo9hnDZ35B3g85eAJh2npXEHzfDoXj2qxNA/lsbQ+lJ+bmwtcDEQRl2HGLBvTCm12IuQKOhc9J7RUsVab3UW7u7uysLCQXfd2Q19eXsr+/r4sLy9nH5pgZ9miX/vw8FDW1tayDY21tdmGFgzaJFm26CUbHx+XwcFB6enpEZ5EdHV1SWdnp3R0dEh7e3uJ2traJOkq0BtqtkqaXYPc5UxPTzvgw8PDMjAwIP39/U6t3NvbKyomJE1yoLUplmRGUywujrVpoltZWXF6yqampmRyctKZgLGxMRkdHXU0MjIiaVOxs1/vtrRzkIQGOJl8fX3dgV9cXHSyOiWqitvAtMl5Rcl0cU1oFCqAU6GR2KiIsDyiLdoUk5ImFd/LMsHVzQEnsQFP0UKPGcIDjo6OrMWkJUm/3sBTcDI5Ftf3OUhuOgHmK0i855FG/YLmxTQyuRYsCo/lmQDWcbdIfGnTPyMBsv9CTWMwAAAAAElFTkSuQmCC) \n",
        "button that shows up next to the text under the heading **This one!**. The button appears when you move your mouse over the text **This one!** Wait for a bit as it loads and goes in circles, and then move on to the third step when it finishes running.\n",
        "\n",
        "Third, click on the upper bar at the top of the page where it says **Runtime**, and then click **Restart and run all**. \n",
        "\n",
        "Your output will appear at the bottom of this page as it trains after a little while. Scroll down below everything else to see new images appear. The images will start by looking like dirt, but the page will eventually ding and show new images as it begins to attempt to match the image to your description. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3cYWzZkrbJd"
      },
      "source": [
        "# Choose Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EbCH111HrTjw"
      },
      "source": [
        "text_input = \" \\\" \" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nD1n0xEBcko"
      },
      "source": [
        "# This one!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b7f768-dfdb-4c97-f556-9eb2a33b76e8"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA version: 10.1\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.7.1+cu101 in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.8.2+cu101 in /usr/local/lib/python3.7/dist-packages (0.8.2+cu101)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (5.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAcixx9Z3XYH"
      },
      "source": [
        "# Top\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJOg9MY7khd"
      },
      "source": [
        "# don't use half of these lol\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import imageio\n",
        "from IPython import display\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import glob\n",
        "\n",
        "from google.colab import output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUOAL4UcytFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226dc031-a76c-4352-df42-55a0c2310874"
      },
      "source": [
        "# were you lucky today?\n",
        "\n",
        "!nvidia-smi -L\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-5b9573de-2890-606d-b9c1-682af905fddf)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeMCHwDdHIcu"
      },
      "source": [
        "# Perceptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm3_VmxpAiB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3568a57d-b0ac-4fa9-f093-d8043c6f9e09"
      },
      "source": [
        "\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "\n",
        "\n",
        "%cd /content/CLIP/\n",
        "\n",
        "!pip install ftfy\n",
        "\n",
        "import os\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "clip.available_models()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load the model\n",
        "perceptor, preprocess = clip.load('ViT-B/32', jit=True)\n",
        "perceptor = perceptor.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'CLIP' already exists and is not an empty directory.\n",
            "/content/CLIP\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (5.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50', 'ViT-B/32']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw3cxCJZDw4w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw0KBLebMywW"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s"
      },
      "source": [
        "# probably don't mess with this unless you're changing generator size\n",
        "im_shape = [512, 512, 3]\n",
        "sideX, sideY, channels = im_shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlIUu0jK3S19"
      },
      "source": [
        "# Define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCj1wr8pOqOe"
      },
      "source": [
        "def displ(img, pre_scaled=True):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1, 2, 0))\n",
        "  if not pre_scaled:\n",
        "    img = scale(img, 48*4, 32*4)\n",
        "  imageio.imwrite(str(3) + '.png', np.array(img))\n",
        "  return display.Image(str(3)+'.png')\n",
        "\n",
        "def gallery(array, ncols=2):\n",
        "    nindex, height, width, intensity = array.shape\n",
        "    nrows = nindex//ncols\n",
        "    assert nindex == nrows*ncols\n",
        "    # want result.shape = (height*nrows, width*ncols, intensity)\n",
        "    result = (array.reshape(nrows, ncols, height, width, intensity)\n",
        "              .swapaxes(1,2)\n",
        "              .reshape(height*nrows, width*ncols, intensity))\n",
        "    return result\n",
        "\n",
        "def card_padded(im, to_pad=3):\n",
        "  return np.pad(np.pad(np.pad(im, [[1,1], [1,1], [0,0]],constant_values=0), [[2,2], [2,2], [0,0]],constant_values=1),\n",
        "            [[to_pad,to_pad], [to_pad,to_pad], [0,0]],constant_values=0)\n",
        "\n",
        "def get_all(img):\n",
        "  img = np.transpose(img, (0,2,3,1))\n",
        "  cards = np.zeros((img.shape[0], sideX+12, sideY+12, 3))\n",
        "  for i in range(len(img)):\n",
        "    cards[i] = card_padded(img[i])\n",
        "  print(img.shape)\n",
        "  cards = gallery(cards)\n",
        "  imageio.imwrite(str(3) + '.png', np.array(cards))\n",
        "  return display.Image(str(3)+'.png')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHB7BcR1zNLZ"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtlDVVMvzMUd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f36e4bf-3afa-4947-8669-dd07ce2a5231"
      },
      "source": [
        "import io\n",
        "import os, sys\n",
        "import requests\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "!pip install git+https://github.com/openai/DALL-E.git\n",
        "\n",
        "\n",
        "from dall_e import map_pixels, unmap_pixels, load_model\n",
        "target_image_size = sideX\n",
        "\n",
        "def preprocess(img):\n",
        "    s = min(img.size)\n",
        "    \n",
        "    if s < target_image_size:\n",
        "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
        "        \n",
        "    r = target_image_size / s\n",
        "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
        "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
        "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
        "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
        "    return map_pixels(img)\n",
        "\n",
        "\n",
        "model = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", 'cuda')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/DALL-E.git\n",
            "  Cloning https://github.com/openai/DALL-E.git to /tmp/pip-req-build-9sou81jd\n",
            "  Running command git clone -q https://github.com/openai/DALL-E.git /tmp/pip-req-build-9sou81jd\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (7.1.2)\n",
            "Collecting blobfile\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/54/9e01c570475b7ea16a4e489bba85b7736d6eac5f6de1fb9081564eb1dfac/blobfile-1.2.3-py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.9MB/s \n",
            "\u001b[?25hCollecting mypy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/f1/e399fde3a4493cb9c014308a0ba90200ee87527b2e89f7f380a6db24b899/mypy-0.902-cp37-cp37m-manylinux2010_x86_64.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (1.19.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (0.9.1+cu101)\n",
            "Collecting xmltodict~=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.7/dist-packages (from blobfile->DALL-E==0.1) (3.0.12)\n",
            "Collecting pycryptodomex~=3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/9d/99a949925b5fc9604cb65219951fd270ef30d0fd4f064d1b363eb8bb5e9b/pycryptodomex-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 36.6MB/s \n",
            "\u001b[?25hCollecting urllib3~=1.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/cd/1e2ec680ec7b09846dc6e605f5a7709dfb9d7128e51a026e7154e18a234e/urllib3-1.26.5-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 56.8MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5.0,>=1.4.0; python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from mypy->DALL-E==0.1) (0.10.2)\n",
            "Collecting mypy-extensions<0.5.0,>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from mypy->DALL-E==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (1.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (57.0.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (8.7.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (21.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->DALL-E==0.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->DALL-E==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->DALL-E==0.1) (2.10)\n",
            "Building wheels for collected packages: DALL-E\n",
            "  Building wheel for DALL-E (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DALL-E: filename=DALL_E-0.1-cp37-none-any.whl size=6011 sha256=60c6834286daee72641f4f8fd0145d9cbd5c66049fbe7e5aec298f7100a84eff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t5nudhzg/wheels/e9/f5/e7/efa7ddb4c5899f6e6ffbbd112b8c7a030872274a5cba9ccf04\n",
            "Successfully built DALL-E\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: xmltodict, pycryptodomex, urllib3, blobfile, typed-ast, mypy-extensions, mypy, DALL-E\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed DALL-E-0.1 blobfile-1.2.3 mypy-0.902 mypy-extensions-0.4.3 pycryptodomex-3.10.1 typed-ast-1.4.3 urllib3-1.26.5 xmltodict-0.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-24617e6fbfa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdall_e\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_pixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munmap_pixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtarget_image_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msideX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sideX' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-aCuJ7SD7Oh"
      },
      "source": [
        "# enc = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "# x = (torch.zeros(1, 3, 512, 512))\n",
        "\n",
        "\n",
        "# z_logits = enc(x)\n",
        "# z = torch.argmax(z_logits, axis=1)\n",
        "# z = torch.nn.functional.one_hot(z, num_classes=enc.vocab_size).permute(0, 3, 1, 2).float()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaocGDQXz3Zx"
      },
      "source": [
        "# Latent coordinate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCh2D8Dt8Xd"
      },
      "source": [
        "\n",
        "class Pars(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pars, self).__init__()\n",
        "\n",
        "        self.normu = torch.nn.Parameter(torch.randn(1, 8192, 64, 64).cuda())\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "\n",
        "      normu = torch.nn.functional.softmax(self.normu.view(1, 8192, -1), dim=-1).view(1, 8192, 64, 64)\n",
        "\n",
        "\n",
        "      return normu\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lats = Pars().cuda()\n",
        "mapper = [lats.normu]\n",
        "optimizer = torch.optim.Adam([{'params': mapper, 'lr': .1}])\n",
        "eps = 0\n",
        "\n",
        "\n",
        "\n",
        "tx = clip.tokenize(text_input)\n",
        "t = perceptor.encode_text(tx.cuda()).detach().clone()\n",
        "\n",
        "\n",
        "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "with torch.no_grad():\n",
        "  mult = 1\n",
        "  al = unmap_pixels(torch.sigmoid(model(lats()).cpu().float())).numpy()\n",
        "  for allls in al:\n",
        "    displ(allls[:3])\n",
        "    print('\\n')\n",
        "  # print(torch.topk(lats().view(1, 8192, -1), k=3, dim=-1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnJZ2bVZ4M9w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjnUIFnbTpy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WztSrRF23Rqg"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EuUz-ICNKUr"
      },
      "source": [
        "#@title Look for images below here { vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "\n",
        "\n",
        "def checkin(loss):\n",
        "  print('''\n",
        "  ##########################################################\n",
        "  ''',\n",
        "        loss, '\\n',itt)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    al = unmap_pixels(torch.sigmoid(model(lats())[:, :3]).cpu().float()).numpy()\n",
        "  for allls in al:\n",
        "    displ(allls)\n",
        "    display.display(display.Image(str(3)+'.png'))\n",
        "    print('\\n')\n",
        "  # the people spoke and they love \"ding\"\n",
        "  output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n",
        "\n",
        "\n",
        "def ascend_txt():\n",
        "  out = unmap_pixels(torch.sigmoid(model(lats())[:, :3].float()))\n",
        "\n",
        "  cutn = 64 # improves quality\n",
        "  p_s = []\n",
        "  for ch in range(cutn):\n",
        "    size = int(sideX*torch.zeros(1,).normal_(mean=.8, std=.3).clip(.5, .98))\n",
        "    offsetx = torch.randint(0, sideX - size, ())\n",
        "    offsety = torch.randint(0, sideX - size, ())\n",
        "    apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n",
        "    apper = torch.nn.functional.interpolate(apper, (224,224), mode='bilinear')\n",
        "    p_s.append(apper)\n",
        "  into = torch.cat(p_s, 0)\n",
        "  # into = torch.nn.functional.interpolate(out, (224,224), mode='nearest')\n",
        "\n",
        "  into = nom(into)\n",
        "\n",
        "  iii = perceptor.encode_image(into)\n",
        "\n",
        "\n",
        "  llls = lats()\n",
        "  lat_l = 0\n",
        "\n",
        "\n",
        "\n",
        "  return [lat_l, 10*-torch.cosine_similarity(t, iii).view(-1, 1).T.mean(1)]\n",
        "\n",
        "def train(i):\n",
        "  loss1 = ascend_txt()\n",
        "  loss = loss1[0] + loss1[1]\n",
        "  loss = loss.mean()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  if itt % 100 == 0:\n",
        "    checkin(loss1)\n",
        "\n",
        "\n",
        "itt = 0\n",
        "for asatreat in range(10000):\n",
        "  train(itt)\n",
        "  itt+=1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJLqKR3tb8E7"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlyowaiV8ZUA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}